# -*- coding: utf-8 -*-
"""Exit_Test@ICT_Food_Demand_Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ahVSy5XzsR2i1kcBAsf2j0hYchfJXit9

# Food Demand Forecasting

## Genpact Machine Learning Hackathon

Your client is a meal delivery company which operates in multiple cities. They have various fulfillment centers in these cities for dispatching meal orders to their customers. The client wants you to help these centers with demand forecasting for upcoming weeks so that these centers will plan the stock of raw materials accordingly.

The replenishment of majority of raw materials is done on weekly basis and since the raw material is perishable, the procurement planning is of utmost importance. Secondly, staffing of the centers is also one area wherein accurate demand forecasts are really helpful. Given the following information, the task is to predict the demand for the next 10 weeks (Weeks: 146-155) for the center-meal combinations in the test set:  

- Historical data of demand for a product-center combination (Weeks: 1 to 145)
- Product(Meal) features such as category, sub-category, current price and discount
- Information for fulfillment center like center area, city information etc.

## Import Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split

"""## Load Dataset"""

df_train = pd.read_csv("train.csv")
df_centre = pd.read_csv("fulfilment_center_info.csv")
df_meal = pd.read_csv("meal_info.csv")
df_test = pd.read_csv("test_QoiMO9B.csv")

df_train.head()

df_train.info()

df_centre.head()

df_centre.info()

df_meal.head()

df_meal.info()

df_test.head()

df_test.info()

"""## Merge the Dataset"""

# Merging train data set into a single dataframe.

df_merge1 = df_train.merge(df_meal, how='left', on='meal_id')
df_merge1

df1_train =df_merge1.merge(df_centre, how='left', on='center_id')
df1_train

# Merged Test data

df_merge2 = df_test.merge(df_meal, how='left', on='meal_id')
df_merge2

df1_test =df_merge2.merge(df_centre, how='left', on='center_id')
df1_test

"""## Data Understanding"""

df1_train.shape

df1_train.size

df1_train.describe().T

df1_train.info()

"""## Duplicate Removal"""

# Checking for duplicate rows
df1_train.duplicated().sum()

# Zero duplicate rows found

# Checking for duplicate columns
df1_train.columns.duplicated().sum()

# Zero duplicate columns found

"""## Checking for Missing Values"""

df1_train.isna().sum()

# No missing values found

"""## Outlier Handling"""

# Seperating numerical and categorical columns
num_df = df1_train.select_dtypes(include="number")
cat_df = df1_train.select_dtypes(include="object_")

num_cols = num_df.columns.tolist()
cat_cols = cat_df.columns.tolist()
print("Numerical Columns: {}".format(num_cols))
print("Categorical Columns: {}".format(cat_cols))

num_df.head()

cat_df.head()

num_df.boxplot()
plt.xticks(rotation=90)
plt.show()

"""### Outlier handling using IQR"""

def remove_outliers(df, column_name):
    q1 = df[column_name].quantile(0.25)
    q3 = df[column_name].quantile(0.75)
    iqr = q3 - q1
    upper_bound = q3 + 1.5 * iqr
    lower_bound = q1 - 1.5 * iqr
    df[column_name] = df[column_name].clip(upper=upper_bound)
    df[column_name] = df[column_name].clip(lower=lower_bound)
    return df[column_name]

for col in num_cols:
 num_df[col] = remove_outliers(num_df, col)

num_df.boxplot()
plt.xticks(rotation=90)
plt.show()

# Seperating numerical and categorical attributes for easy analysis
num_attri = df1_train[["checkout_price","base_price","num_orders","op_area"]]
cat_attri = df1_train[["id","week","center_id","meal_id","emailer_for_promotion","homepage_featured","city_code","region_code","category","cuisine","center_type"]]

"""## Feature Encoding"""

cat_df.describe()

# Frequency encoding for category column on training data

category_frequency = df1_train["category"].value_counts(normalize= True)
df1_train['category_encoded'] = df1_train['category'].map(category_frequency)
df1_train

df1_train = df1_train.drop('category',axis= 1)

# Frequency encoding for category column on testing data

category_frequency1 = df1_test["category"].value_counts(normalize= True)
df1_test['category_encoded'] = df1_test['category'].map(category_frequency1)
df1_test

df1_test = df1_test.drop('category',axis= 1)

cat_df["cuisine"].value_counts()

# Onehot Encoding for cuisine type in training data
df1_train = pd.get_dummies(df1_train, columns=['cuisine'],dtype= int, prefix='cuisine', drop_first=True)
df1_train.head()

# Onehot Encoding for cuisine type in testing data
df1_test = pd.get_dummies(df1_test, columns=['cuisine'],dtype= int, prefix='cuisine', drop_first=True)
df1_test.head()

cat_df["center_type"].value_counts()

# Onehot Encoding for center_type for training
df1_train = pd.get_dummies(df1_train, columns=['center_type'],dtype= int, prefix='Center', drop_first=True)
df1_train.head()

# Onehot Encoding for center_type for testing
df1_test = pd.get_dummies(df1_test, columns=['center_type'],dtype= int, prefix='Center', drop_first=True)
df1_test.head()

"""## Feature Scaling"""

# Min Max scaling in training
min_scaler = MinMaxScaler()
numerical_colms1 = ['checkout_price','base_price','op_area',"week"]
df1_train[numerical_colms1] = min_scaler.fit_transform(df1_train[numerical_colms1])
df1_train

# min max scaling in testing
min_scaler1 = MinMaxScaler()
numerical_colms2 = ['checkout_price','base_price','op_area',"week"]
df1_test[numerical_colms1] = min_scaler1.fit_transform(df1_test[numerical_colms1])
df1_test

# Dropping unnecesary columns
df1_train = df1_train.drop(["id","center_id","meal_id","city_code","region_code"], axis = 1)

df1_train.head()

# Dropping unnecesary columns in testing data
df1_test = df1_test.drop(["id","center_id","meal_id","city_code","region_code"], axis = 1)

df1_test.head()

plt.figure(figsize=(10,8))
sns.heatmap(num_df.corr(),annot= True)

"""## Seperate Features and Labels"""

y_train = df1_train["num_orders"]
X_train = df1_train.drop(["num_orders"], axis = 1)
X_test = df1_test

"""# Model training

## Linear Regression
"""

from sklearn.linear_model import LinearRegression
Linear_model = LinearRegression()

Linear_model.fit(X_train,y_train)
y_pred = Linear_model.predict(X_test)

y_pred

result_df = pd.DataFrame(y_pred)
result_df

Id = df_test["id"]
Id

result_df1 = pd.concat([Id,result_df],axis = 1)

result_df1

result = result_df1.to_csv("final_result.csv")